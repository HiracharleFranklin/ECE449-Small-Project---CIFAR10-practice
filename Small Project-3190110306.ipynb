{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pytorch flow](https://cdn-images-1.medium.com/max/800/1*uZrS4KjAuSJQIJPgOiaJUg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-14T06:19:55.268473Z",
     "iopub.status.busy": "2021-11-14T06:19:55.267878Z",
     "iopub.status.idle": "2021-11-14T06:19:55.273644Z",
     "shell.execute_reply": "2021-11-14T06:19:55.273021Z",
     "shell.execute_reply.started": "2021-11-14T06:19:55.268423Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:19:57.005370Z",
     "iopub.status.busy": "2021-11-14T06:19:57.004813Z",
     "iopub.status.idle": "2021-11-14T06:19:57.009185Z",
     "shell.execute_reply": "2021-11-14T06:19:57.008179Z",
     "shell.execute_reply.started": "2021-11-14T06:19:57.005312Z"
    }
   },
   "outputs": [],
   "source": [
    "run_model1 = False\n",
    "run_model2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-11-14T06:19:59.395664Z",
     "iopub.status.busy": "2021-11-14T06:19:59.395244Z",
     "iopub.status.idle": "2021-11-14T06:19:59.400843Z",
     "shell.execute_reply": "2021-11-14T06:19:59.400212Z",
     "shell.execute_reply.started": "2021-11-14T06:19:59.395592Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_cifar10_data(filename):\n",
    "    with open('../input/cifar10/'+ filename, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    features = batch['data']\n",
    "    labels = batch['labels']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:00.925849Z",
     "iopub.status.busy": "2021-11-14T06:20:00.925268Z",
     "iopub.status.idle": "2021-11-14T06:20:01.226334Z",
     "shell.execute_reply": "2021-11-14T06:20:01.225516Z",
     "shell.execute_reply.started": "2021-11-14T06:20:00.925800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load files\n",
    "batch_1, labels_1 = load_cifar10_data('data_batch_1')\n",
    "batch_2, labels_2 = load_cifar10_data('data_batch_2')\n",
    "batch_3, labels_3 = load_cifar10_data('data_batch_3')\n",
    "batch_4, labels_4 = load_cifar10_data('data_batch_4')\n",
    "batch_5, labels_5 = load_cifar10_data('data_batch_5')\n",
    "\n",
    "test, label_test = load_cifar10_data('test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:02.670440Z",
     "iopub.status.busy": "2021-11-14T06:20:02.669841Z",
     "iopub.status.idle": "2021-11-14T06:20:02.961030Z",
     "shell.execute_reply": "2021-11-14T06:20:02.959946Z",
     "shell.execute_reply.started": "2021-11-14T06:20:02.670391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge files\n",
    "X_train = np.concatenate([batch_1,batch_2,batch_3,batch_4,batch_5], 0)\n",
    "Y_train = np.concatenate([labels_1,labels_2,labels_3,labels_4,labels_5], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:05.065034Z",
     "iopub.status.busy": "2021-11-14T06:20:05.064663Z",
     "iopub.status.idle": "2021-11-14T06:20:05.071165Z",
     "shell.execute_reply": "2021-11-14T06:20:05.070518Z",
     "shell.execute_reply.started": "2021-11-14T06:20:05.064948Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = ('airplane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def return_photo(batch_file):\n",
    "    assert batch_file.shape[1] == 3072\n",
    "    dim = np.sqrt(1024).astype(int)\n",
    "    r = batch_file[:, 0:1024].reshape(batch_file.shape[0], dim, dim, 1)\n",
    "    g = batch_file[:, 1024:2048].reshape(batch_file.shape[0], dim, dim, 1)\n",
    "    b = batch_file[:, 2048:3072].reshape(batch_file.shape[0], dim, dim, 1)\n",
    "    photo = np.concatenate([r,g,b], -1)\n",
    "    return photo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:06.482844Z",
     "iopub.status.busy": "2021-11-14T06:20:06.482504Z",
     "iopub.status.idle": "2021-11-14T06:20:06.856857Z",
     "shell.execute_reply": "2021-11-14T06:20:06.856027Z",
     "shell.execute_reply.started": "2021-11-14T06:20:06.482780Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = return_photo(X_train)\n",
    "X_test = return_photo(test)\n",
    "Y_test = np.array(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:08.206067Z",
     "iopub.status.busy": "2021-11-14T06:20:08.205677Z",
     "iopub.status.idle": "2021-11-14T06:20:08.409704Z",
     "shell.execute_reply": "2021-11-14T06:20:08.408704Z",
     "shell.execute_reply.started": "2021-11-14T06:20:08.206000Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_image(number, file, label, pred=None):\n",
    "    fig = plt.figure(figsize = (3,2))\n",
    "    #img = return_photo(batch_file)\n",
    "    plt.imshow(file[number])\n",
    "    if pred is None:\n",
    "        plt.title(classes[label[number]])\n",
    "    else:\n",
    "        plt.title('Label_true: ' + classes[label[number]] + '\\nLabel_pred: ' + classes[pred[number]])\n",
    "    \n",
    "plot_image(12345, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:10.149520Z",
     "iopub.status.busy": "2021-11-14T06:20:10.148909Z",
     "iopub.status.idle": "2021-11-14T06:20:10.467684Z",
     "shell.execute_reply": "2021-11-14T06:20:10.466680Z",
     "shell.execute_reply.started": "2021-11-14T06:20:10.149468Z"
    }
   },
   "outputs": [],
   "source": [
    "# The cifar-10 is designed to balance distribution that the counts for each classification are 5000\n",
    "import seaborn as sns\n",
    "sns.countplot(Y_train)\n",
    "hist_Y_train = pd.Series(Y_train).groupby(Y_train).count()\n",
    "print(hist_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:13.696545Z",
     "iopub.status.busy": "2021-11-14T06:20:13.695895Z",
     "iopub.status.idle": "2021-11-14T06:20:13.703094Z",
     "shell.execute_reply": "2021-11-14T06:20:13.702397Z",
     "shell.execute_reply.started": "2021-11-14T06:20:13.696484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final check for dimensions before pre-pocessing\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:17.062406Z",
     "iopub.status.busy": "2021-11-14T06:20:17.061767Z",
     "iopub.status.idle": "2021-11-14T06:20:17.176303Z",
     "shell.execute_reply": "2021-11-14T06:20:17.175627Z",
     "shell.execute_reply.started": "2021-11-14T06:20:17.062342Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the validation set out\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:18.604886Z",
     "iopub.status.busy": "2021-11-14T06:20:18.604269Z",
     "iopub.status.idle": "2021-11-14T06:20:18.616367Z",
     "shell.execute_reply": "2021-11-14T06:20:18.615343Z",
     "shell.execute_reply.started": "2021-11-14T06:20:18.604828Z"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "### Prepare for training & testing dataset. Define dataset class.\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# define the random seed for reproducible result\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "class CIFAR10_from_array(Dataset): \n",
    "    def __init__(self, data, label, transform=None):\n",
    "        ##############################################\n",
    "        ### Initialize paths, transforms, and so on\n",
    "        ##############################################\n",
    "        #self.data = torch.from_numpy(data).float()\n",
    "        #self.label = torch.from_numpy(label).long()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.img_shape = data.shape\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ##############################################\n",
    "        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (torchvision.Transform).\n",
    "        # 3. Return the data (e.g. image and label)\n",
    "        ##############################################\n",
    "        \n",
    "        img = Image.fromarray(self.data[index])\n",
    "        label = self.label[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            img = img_to_tensor(img)\n",
    "            #label = torch.from_numpy(label).long()\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        ##############################################\n",
    "        ### Indicate the total size of the dataset\n",
    "        ##############################################\n",
    "        return len(self.data)\n",
    "    \n",
    "    def plot_image(self, number):\n",
    "        file = self.data\n",
    "        label = self.label\n",
    "        fig = plt.figure(figsize = (3,2))\n",
    "        #img = return_photo(batch_file)\n",
    "        plt.imshow(file[number])\n",
    "        plt.title(classes[label[number]])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:20.172778Z",
     "iopub.status.busy": "2021-11-14T06:20:20.172450Z",
     "iopub.status.idle": "2021-11-14T06:20:20.177112Z",
     "shell.execute_reply": "2021-11-14T06:20:20.176094Z",
     "shell.execute_reply.started": "2021-11-14T06:20:20.172718Z"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR10_from_url(Dataset): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:22.223597Z",
     "iopub.status.busy": "2021-11-14T06:20:22.222945Z",
     "iopub.status.idle": "2021-11-14T06:20:33.665186Z",
     "shell.execute_reply": "2021-11-14T06:20:33.664384Z",
     "shell.execute_reply.started": "2021-11-14T06:20:22.223548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize for R, G, B with img = img - mean / std\n",
    "def normalize_dataset(data):\n",
    "    mean = data.mean(axis=(0,1,2)) / 255.0\n",
    "    std = data.std(axis=(0,1,2)) / 255.0\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "    return normalize\n",
    "\n",
    "\n",
    "# \n",
    "train_transform_aug = transforms.Compose([\n",
    "###\n",
    "#insert your code here  \n",
    "#implement some data augmentation methods here\n",
    "#for example, transforms.Resize((40, 40)),\n",
    "    \n",
    "###\n",
    "    transforms.Resize((40, 40)),       #First resize it to a given size 40by40\n",
    "    transforms.RandomCrop((32, 32)),   #Then cut the image back to 32by32 at a random place\n",
    "    transforms.RandomHorizontalFlip(), #Flip the image horizontally with 0.5 probability\n",
    "    transforms.RandomVerticalFlip(),   #Flip the image vertically with 0.5 probability\n",
    "    transforms.RandomRotation(15),     #Rotate the image\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    normalize_dataset(X_train)\n",
    "])\n",
    "\n",
    "# Also use X_train in normalize since train/val sets should have same distribution\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_dataset(X_train)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_dataset(X_test)\n",
    "])\n",
    "\n",
    "trainset = CIFAR10_from_array(data=X_train_split, label=Y_train_split, transform=train_transform_aug)\n",
    "valset = CIFAR10_from_array(data=X_val_split, label=Y_val_split, transform=val_transform)\n",
    "testset = CIFAR10_from_array(data=X_test, label=Y_test, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:36.688575Z",
     "iopub.status.busy": "2021-11-14T06:20:36.688192Z",
     "iopub.status.idle": "2021-11-14T06:20:36.707423Z",
     "shell.execute_reply": "2021-11-14T06:20:36.706289Z",
     "shell.execute_reply.started": "2021-11-14T06:20:36.688496Z"
    }
   },
   "outputs": [],
   "source": [
    "print('data shape check')\n",
    "print('training set:'.ljust(20) + '{}'.format(trainset.img_shape))\n",
    "print('validation set:'.ljust(20) + '{}'.format(valset.img_shape))\n",
    "print('testing set:'.ljust(20) + '{}'.format(testset.img_shape))\n",
    "print('label numbers:'.ljust(20) + '{}'.format(len(set(trainset.label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:38.945854Z",
     "iopub.status.busy": "2021-11-14T06:20:38.945305Z",
     "iopub.status.idle": "2021-11-14T06:20:38.957834Z",
     "shell.execute_reply": "2021-11-14T06:20:38.956815Z",
     "shell.execute_reply.started": "2021-11-14T06:20:38.945785Z"
    }
   },
   "outputs": [],
   "source": [
    "# put into the data loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 1\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "\n",
    "val_loader = DataLoader(dataset=valset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "test_loader = DataLoader(dataset=testset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:40.867031Z",
     "iopub.status.busy": "2021-11-14T06:20:40.866648Z",
     "iopub.status.idle": "2021-11-14T06:20:40.981554Z",
     "shell.execute_reply": "2021-11-14T06:20:40.980667Z",
     "shell.execute_reply.started": "2021-11-14T06:20:40.866949Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs, lbls = iter(train_loader).next()\n",
    "print ('Size of image:', imgs.size())  # batch_size*3*224*224\n",
    "print ('Type of image:', imgs.dtype)   # float32\n",
    "print ('Size of label:', lbls.size())  # batch_size\n",
    "print ('Type of label:', lbls.dtype)   # int64(long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "### Model with out augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:20:44.780544Z",
     "iopub.status.busy": "2021-11-14T06:20:44.780185Z",
     "iopub.status.idle": "2021-11-14T06:20:44.785731Z",
     "shell.execute_reply": "2021-11-14T06:20:44.784691Z",
     "shell.execute_reply.started": "2021-11-14T06:20:44.780478Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:34:30.146982Z",
     "iopub.status.busy": "2021-11-14T06:34:30.146684Z",
     "iopub.status.idle": "2021-11-14T06:34:30.155295Z",
     "shell.execute_reply": "2021-11-14T06:34:30.154504Z",
     "shell.execute_reply.started": "2021-11-14T06:34:30.146932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class Net(nn.Module):\n",
    "        #implement your own network here\n",
    "        #you can use any simple CNN structure learned from class\n",
    "        #for example, LeNet\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:34:31.364659Z",
     "iopub.status.busy": "2021-11-14T06:34:31.364303Z",
     "iopub.status.idle": "2021-11-14T06:34:31.387255Z",
     "shell.execute_reply": "2021-11-14T06:34:31.386598Z",
     "shell.execute_reply.started": "2021-11-14T06:34:31.364594Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model training\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def update_info(idx, length, epoch_loss, acc, mode):\n",
    "    \n",
    "    if length >= 250:\n",
    "        update_size = int(length/250)\n",
    "    else:\n",
    "        update_size = 5\n",
    "    \n",
    "    if idx % update_size == 0 and idx != 0:\n",
    "        #print ('=', end=\"\")        \n",
    "        finish_rate = idx/length * 100\n",
    "        print (\"\\r   {} progress: {:.2f}%  ......  loss: {:.4f} , acc: {:.4f}\".\n",
    "               format(mode, finish_rate, epoch_loss/idx, acc), end=\"\", flush=True)\n",
    "        \n",
    "\n",
    "def val_per_epoch(model, loss_fn, dataloader, verbose):\n",
    "    # In validation, we only compute loss value\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    acc = 0.0\n",
    "    val_size = 0\n",
    "    with torch.no_grad(): \n",
    "        for i, (feature, target) in enumerate(dataloader):\n",
    "            \n",
    "            #feature, target = feature.to(device), target.to(device)\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            output = model(feature) #outputs.data.shape= batches_num * num_class\n",
    "            \n",
    "            #compute acc\n",
    "            _, pred = torch.max(output.data, dim=1) \n",
    "            correct = (pred == target).sum().item() #convert to number\n",
    "            val_size += target.size(0)\n",
    "            acc += correct\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            idx = i\n",
    "            length = len(dataloader)\n",
    "            \n",
    "            #display progress\n",
    "            if verbose:\n",
    "                update_info(idx, length, epoch_loss, acc/val_size, 'validating')\n",
    "                \n",
    "        acc = acc/val_size\n",
    "    print('')\n",
    "    return epoch_loss/len(dataloader), acc\n",
    "\n",
    "\n",
    "def train_per_epoch(model, loss_fn, dataloader, optimizer, verbose): \n",
    "    #train mode\n",
    "    model.train()\n",
    "    \n",
    "    #initialize loss\n",
    "    epoch_loss = 0.0\n",
    "    acc = 0.0\n",
    "    train_size = 0\n",
    "    \n",
    "    for i, (feature, target) in enumerate(dataloader):\n",
    "        #feature, target = feature.to(device), target.to(device)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            feature = feature.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        #set zero to the parameter gradients for initialization\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature)\n",
    "        loss = loss_fn(output, target)\n",
    "        \n",
    "        \n",
    "        #compute acc\n",
    "        _, pred = torch.max(output.data, dim=1) \n",
    "        correct = (pred == target).sum().item() #convert to number\n",
    "        train_size += target.size(0)\n",
    "        acc += correct\n",
    "        \n",
    "        #compute current loss. Loss is a 0-dim tensor, so use tensor.item() to get the scalar value\n",
    "        epoch_loss += loss.item()  \n",
    "        \n",
    "        #backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        #this represents one update on the weight/bias for a mini-batch(16 images in our case): \n",
    "        #weights[k] + alpha * d_weights[k]\n",
    "        optimizer.step()\n",
    "        \n",
    "        #show the update information\n",
    "        idx = i\n",
    "        length = len(dataloader)\n",
    "        \n",
    "        #display progress\n",
    "        if verbose:\n",
    "            update_info(idx, length, epoch_loss, acc/train_size, '  training')\n",
    "            \n",
    "    acc = acc/train_size\n",
    "    print('') \n",
    "    return epoch_loss/len(dataloader), acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_training(num_epochs, model, loss_fn, train_loader, optimizer, val_loader=None, verbose=True):\n",
    "    \n",
    "    train_batch_num = len(train_loader)\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "    history['val_acc'] = []\n",
    "    \n",
    "    if val_loader is not None:\n",
    "        \n",
    "        val_batch_num = len(val_loader)\n",
    "        \n",
    "        print('Total Sample: Train on {} samples, validate on {} samples.'.\n",
    "             format(trainset.img_shape[0], valset.img_shape[0]))\n",
    "        \n",
    "        print(' Total Batch: Train on {} batches, validate on {} batches. {} samples/minibatch \\n'.\n",
    "         format(train_batch_num, val_batch_num, batch_size))\n",
    "    \n",
    "    else:\n",
    "        print('Total Sample: Train on {} samples.'.\n",
    "             format(train_batch_num*batch_size))\n",
    "        \n",
    "        print(' Total Batch: Train on {} batches, {} samples/minibatch \\n'.\n",
    "         format(train_batch_num, batch_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        train_loss, train_acc = train_per_epoch(model, loss_fn, train_loader, optimizer, verbose=verbose)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        \n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = val_per_epoch(model, loss_fn, val_loader, verbose=verbose)\n",
    "            print('\\n        Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(train_loss,val_loss))\n",
    "            print('         Training acc: {:.4f},  Validation acc: {:.4f}\\n'.format(train_acc,val_acc))\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "                        \n",
    "        else:\n",
    "            print('\\n        Training Loss: {:.4f}\\n'.format(train_loss))\n",
    "            print('\\n         Training acc: {:.4f}\\n'.format(train_acc))\n",
    "        \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:34:32.800992Z",
     "iopub.status.busy": "2021-11-14T06:34:32.800445Z",
     "iopub.status.idle": "2021-11-14T06:39:34.403363Z",
     "shell.execute_reply": "2021-11-14T06:39:34.402119Z",
     "shell.execute_reply.started": "2021-11-14T06:34:32.800941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training/Validating the model\n",
    "classes = ('airplane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def lr_decay(parm):\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    print(net)\n",
    "    print('=================================================================')\n",
    "\n",
    "    #insert your code here\n",
    "    #implement criterion(also known as loss funtion) and optimizer here\n",
    "    #we suggest you to use CrossEntropyLoss and Adam\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() #loss function\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    #training and validating\n",
    "    hist1 = model_training(num_epochs, net, criterion, train_loader, optimizer, val_loader, verbose=True)\n",
    "\n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    print(net)\n",
    "    print('=================================================================')\n",
    "\n",
    "    #insert your code here\n",
    "    #implement criterion(also known as loss funtion) and optimizer here\n",
    "    #we suggest you to use CrossEntropyLoss and Adam\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() #loss function\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    #training and validating\n",
    "    hist2 = model_training(num_epochs, net, criterion, train_loader, optimizer, val_loader, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:02.129613Z",
     "iopub.status.busy": "2021-11-14T06:43:02.129188Z",
     "iopub.status.idle": "2021-11-14T06:43:02.137719Z",
     "shell.execute_reply": "2021-11-14T06:43:02.136839Z",
     "shell.execute_reply.started": "2021-11-14T06:43:02.129556Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img    # unnormalize\n",
    "    #print(img)\n",
    "    npimg = img.numpy()\n",
    "    print(np.transpose(npimg, (1, 2, 0)).shape)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        plot_image(i, images.permute(0, 2, 3, 1).numpy(), labels.numpy())\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  for j in range(5)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:08.820546Z",
     "iopub.status.busy": "2021-11-14T06:43:08.820207Z",
     "iopub.status.idle": "2021-11-14T06:43:12.229918Z",
     "shell.execute_reply": "2021-11-14T06:43:12.228818Z",
     "shell.execute_reply.started": "2021-11-14T06:43:08.820491Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_testing(model, loss_fn, dataloader, verbose=True):\n",
    "    Y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0.0\n",
    "    acc = 0.0\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (feature, target) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            outputs = model(feature)  #outputs.data.shape= batches_num * num_class\n",
    "            \n",
    "            #compute acc\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            correct = (pred == target).sum().item() #convert to number\n",
    "            test_size += target.size(0)\n",
    "            #print(test_size)\n",
    "            acc += correct\n",
    "            \n",
    "            loss = loss_fn(outputs, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            idx = i\n",
    "            length = len(dataloader)\n",
    "\n",
    "\n",
    "            #if torch.cuda.is_available():\n",
    "            #    pred = pred.cuda()\n",
    "            \n",
    "            #Pred labels \n",
    "            Y_pred += pred.cpu().numpy().tolist()\n",
    "            \n",
    "            if verbose:\n",
    "                update_info(idx, length, epoch_loss, acc/test_size, 'testing')    \n",
    "            \n",
    "    acc = acc/test_size\n",
    "    print('\\n\\n Accuracy of the network on the {} test images: {}%'.format(test_size, 100*acc))\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    Y_pred1 = model_testing(net, criterion, test_loader, True)\n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    Y_pred2 = model_testing(net, criterion, test_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:15.498087Z",
     "iopub.status.busy": "2021-11-14T06:43:15.497739Z",
     "iopub.status.idle": "2021-11-14T06:43:15.505341Z",
     "shell.execute_reply": "2021-11-14T06:43:15.504483Z",
     "shell.execute_reply.started": "2021-11-14T06:43:15.498034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "def loss_acc_plt(history):\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    ax[0].plot(history['train_loss'], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history['train_acc'], color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:19.670502Z",
     "iopub.status.busy": "2021-11-14T06:43:19.670142Z",
     "iopub.status.idle": "2021-11-14T06:43:20.154107Z",
     "shell.execute_reply": "2021-11-14T06:43:20.152759Z",
     "shell.execute_reply.started": "2021-11-14T06:43:19.670438Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    loss_acc_plt(hist1)\n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    loss_acc_plt(hist2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:25.390998Z",
     "iopub.status.busy": "2021-11-14T06:43:25.390476Z",
     "iopub.status.idle": "2021-11-14T06:43:26.916539Z",
     "shell.execute_reply": "2021-11-14T06:43:26.915553Z",
     "shell.execute_reply.started": "2021-11-14T06:43:25.390946Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    for i in range(10):\n",
    "        plot_image(i, test_loader.dataset.data, test_loader.dataset.label, Y_pred1)\n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    for i in range(10):\n",
    "        plot_image(i, test_loader.dataset.data, test_loader.dataset.label, Y_pred2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:33.641961Z",
     "iopub.status.busy": "2021-11-14T06:43:33.641670Z",
     "iopub.status.idle": "2021-11-14T06:43:33.679327Z",
     "shell.execute_reply": "2021-11-14T06:43:33.678597Z",
     "shell.execute_reply.started": "2021-11-14T06:43:33.641923Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    cm = confusion_matrix(Y_test, Y_pred1)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    cm = confusion_matrix(Y_test, Y_pred2)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:41.919480Z",
     "iopub.status.busy": "2021-11-14T06:43:41.918948Z",
     "iopub.status.idle": "2021-11-14T06:43:43.016898Z",
     "shell.execute_reply": "2021-11-14T06:43:43.015688Z",
     "shell.execute_reply.started": "2021-11-14T06:43:41.919233Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.colorbar()\n",
    "n_classes = cm.shape[0]\n",
    "range_class = range(n_classes)\n",
    "tick_marks = np.arange(len(range_class))\n",
    "plt.xticks(tick_marks, range_class, rotation=-45, fontsize=14)\n",
    "plt.yticks(tick_marks, range_class, fontsize=14)\n",
    "plt.xlabel('Predicted label', fontsize=14)\n",
    "plt.ylabel('True label', fontsize=14)\n",
    "\n",
    "for i in range_class:\n",
    "    for j in range_class:        \n",
    "        plt.text(j, i, cm[i,j], horizontalalignment=\"center\", fontsize=14, \n",
    "                color=\"white\" if i==j else \"black\")\n",
    "plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T06:43:47.692731Z",
     "iopub.status.busy": "2021-11-14T06:43:47.692444Z",
     "iopub.status.idle": "2021-11-14T06:43:47.726211Z",
     "shell.execute_reply": "2021-11-14T06:43:47.725367Z",
     "shell.execute_reply.started": "2021-11-14T06:43:47.692694Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(classes)):\n",
    "    correct = ((Y_test == i)*1) * ((np.array(Y_pred2) == Y_test)*1)\n",
    "    print('{}, {}: '.rjust(10).format(i, classes[i]) + '{}%'.\n",
    "          format(100*correct.sum()/Y_test[Y_test == i].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
